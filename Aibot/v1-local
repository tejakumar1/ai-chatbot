import os
import uuid
import time
import json
import streamlit as st
from dotenv import load_dotenv
from user_agents import parse as ua_parse
import pandas as pd
from opik import track, configure
import openai

# ---------------- Load Environment ----------------
load_dotenv()

# Azure OpenAI configuration
openai.api_key = os.getenv("AZURE_OPENAI_API_KEY")
openai.api_base = os.getenv("AZURE_OPENAI_ENDPOINT")
openai.api_type = "azure"
openai.api_version = os.getenv("AZURE_OPENAI_API_VERSION", "2025-01-01-preview")
deployment_name = os.getenv("GPT_DEPLOYMENT_NAME", "gpt-4o-mini")

# ---------------- Configure Local OPIK ----------------
configure(use_local=True)
TRACE_FILE = "local_traces.json"

if os.path.exists(TRACE_FILE):
    with open(TRACE_FILE, "r") as f:
        saved_traces = json.load(f)
else:
    saved_traces = []

# ---------------- Streamlit UI ----------------
st.set_page_config(page_title="AI Multi-Agent Dashboard", layout="wide")
st.title("ü§ñ AI Multi-Agent (Local Traces)")
st.caption("All traces are stored locally and viewable below.")

if "messages" not in st.session_state:
    st.session_state.messages = []

if "session_id" not in st.session_state:
    st.session_state.session_id = str(uuid.uuid4())
    st.info(f"Session ID: `{st.session_state.session_id[:8]}...`")

# ---------------- Helper Functions ----------------
def get_user_metadata():
    metadata = {}
    try:
        ua_string = st.runtime.scriptrunner.script_run_context.request.headers.get("user-agent", "")
        user_agent = ua_parse(ua_string)
        metadata["device"] = f"{user_agent.device.family} | {user_agent.os.family} | {user_agent.browser.family}"
    except Exception:
        metadata["device"] = "unknown"
    try:
        import requests
        ip_res = requests.get("https://ipinfo.io/json").json()
        metadata.update({
            "ip": ip_res.get("ip"),
            "city": ip_res.get("city"),
            "region": ip_res.get("region"),
            "country": ip_res.get("country")
        })
    except Exception:
        metadata.update({"ip":"unknown","city":"unknown","region":"unknown","country":"unknown"})
    return metadata

def python_calculator(code: str) -> str:
    try:
        from asteval import Interpreter
        aeval = Interpreter()
        return str(aeval(code))
    except Exception as e:
        return f"Calculator Error: {e}"

def file_reader(file_path: str) -> str:
    try:
        with open(file_path, "r") as f:
            return f.read()
    except Exception as e:
        return f"File Read Error: {e}"

def generate_response(prompt: str) -> str:
    """
    Generates a real AI response using Azure OpenAI Chat Completions.
    """
    try:
        response = openai.chat.completions.create(
            model=deployment_name, 
            messages=[{"role": "user", "content": prompt}]
        )
        return response.choices[0].message.content
    except Exception as e:
        return f"OpenAI call error: {e}"

# ---------------- Display Previous Messages ----------------
for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])
        if msg.get("trace_id"):
            st.markdown(f"üß† Trace ID: `{msg['trace_id']}`")

# ---------------- Chat Input ----------------
if prompt := st.chat_input("Ask a question..."):
    with st.chat_message("user"):
        st.markdown(prompt)
    st.session_state.messages.append({"role": "user", "content": prompt})

    metadata = get_user_metadata()
    metadata["session_id"] = st.session_state.session_id
    metadata["turn"] = len(st.session_state.messages)//2 + 1

    @track(project_name="local_project", metadata={"prompt": prompt, **metadata})
    def process_turn():
        response_texts = []

        if "calculate" in prompt.lower() or "math" in prompt.lower():
            response_texts.append(f"Calculator Result: {python_calculator(prompt)}")
        if "read file" in prompt.lower() or "open file" in prompt.lower():
            response_texts.append(f"File Content:\n{file_reader('example.txt')}")
        
        # Real AI-generated response
        response_texts.append(f"AI Response:\n{generate_response(prompt)}")
        return "\n\n".join(response_texts)

    combined_response = process_turn()
    trace_id = str(uuid.uuid4())[:8]

    with st.chat_message("assistant"):
        st.markdown(combined_response)
        st.markdown(f"üß† Trace ID: `{trace_id}`")

    # Save traces
    trace_entry_user = {"role":"user","content":prompt,"trace_id":trace_id,"metadata":metadata,"timestamp":time.time()}
    trace_entry_assistant = {"role":"assistant","content":combined_response,"trace_id":trace_id,"metadata":metadata,"timestamp":time.time()}
    saved_traces.extend([trace_entry_user, trace_entry_assistant])
    st.session_state.messages.append({"role":"assistant","content":combined_response,"trace_id":trace_id})

    with open(TRACE_FILE, "w") as f:
        json.dump(saved_traces, f, indent=2)

# ---------------- Local Trace Dashboard ----------------
st.divider()
st.subheader("üìä Local Trace Dashboard (All Sessions)")

if saved_traces:
    df = pd.DataFrame(saved_traces)
    df["time"] = pd.to_datetime(df["timestamp"], unit="s").dt.strftime("%Y-%m-%d %H:%M:%S")
    df["metadata"] = df["metadata"].apply(lambda x: x if isinstance(x, dict) else {})

    session_ids = df["metadata"].apply(lambda x: x.get("session_id", "unknown")).unique()
    sessions = ["All"] + sorted(session_ids)
    roles = ["All", "user", "assistant"]

    selected_session = st.selectbox("Filter by Session ID", sessions)
    selected_role = st.selectbox("Filter by Role", roles)
    search_text = st.text_input("Search messages...")

    filtered_df = df.copy()
    if selected_session != "All":
        filtered_df = filtered_df[filtered_df["metadata"].apply(lambda x: x.get("session_id", "unknown")) == selected_session]
    if selected_role != "All":
        filtered_df = filtered_df[filtered_df["role"] == selected_role]
    if search_text:
        filtered_df = filtered_df[filtered_df["content"].str.contains(search_text, case=False, na=False)]

    for idx, row in filtered_df.iterrows():
        with st.expander(f"{row['time']} | {row['role']} | Trace ID: {row['trace_id']}"):
            st.markdown(row["content"])
            st.markdown(f"Metadata: `{row['metadata']}`")
else:
    st.info("No traces recorded yet.")

# ---------------- Download Button ----------------
st.divider()
st.subheader("üíæ Download All Traces")
if os.path.exists(TRACE_FILE):
    with open(TRACE_FILE, "r") as f:
        json_data = f.read()
    st.download_button("Download Traces as JSON", data=json_data, file_name="local_traces.json")
else:
    st.info("No trace file found yet.")





previous----

import streamlit as st
import opik
from opik import track, opik_context, Opik 
import os
import uuid 
import time

# --- Third-Party LLM & AGENT Imports (Updated) ---
from langchain_openai import AzureChatOpenAI 
from langchain_core.messages import HumanMessage
from langchain.agents import AgentExecutor, create_openai_functions_agent
from langchain import hub
from langchain_experimental.tools.python.tool import PythonREPLTool # ‚úÖ Replaces Calculator
from langchain_core.tools import Tool
# ---------------------------------------------------

# --- 1. Opik and Azure Environment Setup (Unchanged) ---

# AZURE KEYS 
AZURE_OPENAI_API_KEY = os.getenv("AZURE_OPENAI_API_KEY", "726WoJruf3CtnvJbpF4YQarspGo2wvAWQv2dGc8J6Ihx4mPIKM2qJQQJ99BDACHYHv6XJ3w3AAAAACOGIsap")
AZURE_OPENAI_ENDPOINT = os.getenv("AZURE_OPENAI_ENDPOINT", "https://jon-m900oi9e-eastus2.cognitiveservices.azure.com/")
AZURE_OPENAI_API_VERSION = os.getenv("AZURE_OPENAI_API_VERSION", "2025-01-01-preview")
GPT_DEPLOYMENT_NAME = os.getenv("GPT_DEPLOYMENT_NAME", "gpt-4o-mini") 

# OPIK KEYS
OPIK_WORKSPACE = os.getenv("OPIK_WORKSPACE_NAME", "azure-chat-dashboard-opik")
OPIK_PROJECT = os.getenv("OPIK_PROJECT_NAME", "Azure-Chat-Dashboard-Opik")

# Initialize Opik client globally
opik_client = None
try:
    opik_client = Opik()
    st.sidebar.success(f"Opik client connected. Target: {OPIK_WORKSPACE}/{OPIK_PROJECT}")
except Exception as e:
    st.sidebar.error(f"FATAL: Opik client failed to initialize. Check OPIK_API_KEY. Error: {e}")

# --- Initialize LLM and Agent Components ---
llm = None
agent_executor = None
tools = []

try:
    llm = AzureChatOpenAI( 
        openai_api_version=AZURE_OPENAI_API_VERSION,
        azure_endpoint=AZURE_OPENAI_ENDPOINT,
        openai_api_key=AZURE_OPENAI_API_KEY,
        azure_deployment=GPT_DEPLOYMENT_NAME,
        temperature=0, 
        streaming=False
    )
    st.sidebar.caption(f"LLM: {GPT_DEPLOYMENT_NAME} connected.")

    # ‚úÖ Use PythonREPLTool for calculator-like functionality
    tools = [PythonREPLTool()]
    
    # 2. Get the official Agent Prompt
    prompt_template = hub.pull("hwchase17/openai-functions-agent")
    
    # 3. Create the Agent
    agent = create_openai_functions_agent(llm, tools, prompt_template)

    # 4. Create the Executor
    agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)
    st.sidebar.caption("Agent Executor initialized with Python REPL tool.")

except Exception as e:
    st.sidebar.error(f"FATAL: Agent/LLM failed to initialize. Error: {e}")


# --- 2. Opik-Tracked Agent Function ---
@track(project_name=OPIK_PROJECT)
def generate_llm_response(prompt: str, thread_id: str) -> tuple[str, str | None]:
    """
    Calls the Agent Executor with the user's prompt.
    """
    llm_response = "Error: Agent or LLM not initialized."
    trace_url = None
    
    time.sleep(0.5)
    
    if agent_executor:
        try:
            response = agent_executor.invoke({"input": prompt}) 
            llm_response = response["output"] 
        except Exception as e:
            llm_response = f"Agent Execution Error: {e}"

    # --- Opik Tracing Logic ---
    try:
        current_trace_data = opik_context.get_current_trace_data()
        trace_id = current_trace_data.id 
        project_name = current_trace_data.project_name if current_trace_data.project_name else OPIK_PROJECT
        opik_base_url = "https://www.comet.com/opik"
        trace_url = f"{opik_base_url}/{OPIK_WORKSPACE}/projects/{project_name}/traces/{trace_id}"
        
    except Exception as e:
        st.warning(f"‚ö†Ô∏è Tracing Error: Could not generate trace URL. Error: {e}")

    return llm_response, trace_url

# ---------------------------------------------------------------------

# --- 3. Streamlit Application ---

st.set_page_config(page_title="Azure Opik Agent Chatbot", layout="centered")
st.title("Azure Agent (Traced by Opik) Chat ü§ñ")
st.caption(f"Traces logged to Opik Workspace: **{OPIK_WORKSPACE}** / Project: **{OPIK_PROJECT}**")

# Initialize chat history and unique session/thread ID
if "messages" not in st.session_state:
    st.session_state.messages = []
    
if "opik_thread_id" not in st.session_state:
    st.session_state.opik_thread_id = str(uuid.uuid4())
    st.info(f"Opik Thread ID: `{st.session_state.opik_thread_id[:8]}...` (Groups conversation turns)")

# Display chat messages from history
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])
        if message.get("trace_url"):
            st.markdown("---") 
            st.markdown(f"**üî¨ Trace:** [View in Opik]({message['trace_url']})")

# React to user input
if prompt := st.chat_input("Ask a question (e.g., What is 123*45?)"):
    
    # 1. Display User Message
    with st.chat_message("user"):
        st.markdown(prompt)
    st.session_state.messages.append({"role": "user", "content": prompt})
    
    # 2. Get LLM Response and Opik Trace URL
    llm_response, trace_link = generate_llm_response(prompt, st.session_state.opik_thread_id)
    
    # 3. Flush Opik client to send trace data
    if opik_client:
        try:
            opik_client.flush()
        except Exception as e:
            st.warning(f"Could not flush Opik client. Error: {e}")

    # 4. Display Assistant Message
    with st.chat_message("assistant"):
        st.markdown(llm_response)
        if trace_link:
            st.markdown("---") 
            st.markdown(f"**üî¨ Trace:** [View in Opik]({trace_link})")
            
    # 5. Save assistant response and trace URL to history
    st.session_state.messages.append({
        "role": "assistant", 
        "content": llm_response,
        "trace_url": trace_link
    })
